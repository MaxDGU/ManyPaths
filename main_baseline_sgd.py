import sys
import os
import argparse
import numpy as np
import torch
import torch.nn as nn
from torch.utils.data import DataLoader
import random
import pandas as pd
import time

# Assuming these can be imported directly and sys.path is okay from your cluster setup
from initialization import init_dataset, init_model, init_misc
from evaluation import evaluate_model_on_task # A new evaluation helper might be needed
from utils import set_random_seeds, get_collate
from generate_concepts import PCFG_DEFAULT_MAX_DEPTH # For default value
from constants import DEFAULT_INDEX # Assuming this is relevant for model init if not doing hyper_search

# Directory for saving task-specific baseline models
BASELINE_MODELS_SAVE_DIR = "saved_models/baseline_sgd_task_models"
BASELINE_RESULTS_DIR = "results/baseline_sgd"
SAVED_DATASETS_DIR = "saved_datasets" # Base directory where MAML datasets are saved

def train_baseline_on_task(model, support_X, support_y, criterion, optimizer, device, num_train_steps):
    model.train()
    losses = []
    for step in range(num_train_steps):
        optimizer.zero_grad()
        pred = model(support_X)
        loss = criterion(pred, support_y)
        loss.backward()
        optimizer.step()
        losses.append(loss.item())
    return losses

def main(
    seed: int = 0,
    experiment: str = "concept",
    m: str = "mlp",
    data_type: str = "bits",
    num_tasks_to_evaluate: int = 100,
    num_sgd_steps_per_task: int = 100,
    lr: float = 1e-3,
    num_concept_features: int = 8,
    pcfg_max_depth: int = PCFG_DEFAULT_MAX_DEPTH,
    save_each_task_model: bool = True,
    hyper_index: int = DEFAULT_INDEX,
    verbose: bool = False,
    run_name: str = "run_baseline",
    use_fixed_eval_set: str = None,
    maml_experiment_source: str = "concept",
    maml_model_arch_source: str = "mlp",
    maml_data_type_source: str = "bits",
    maml_num_concept_features_source: int = 8,
    maml_pcfg_max_depth_source: int = PCFG_DEFAULT_MAX_DEPTH,
    maml_seed_source: int = 0,
    maml_skip_param_source: int = 1,
    maml_alphabet_source: str = "asian"
):
    device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
    print(f"Device: {device}")
    set_random_seeds(seed)

    # --- Output Directory Setup ---
    current_run_results_dir = os.path.join(BASELINE_RESULTS_DIR, run_name)
    current_run_models_dir = os.path.join(BASELINE_MODELS_SAVE_DIR, run_name)
    os.makedirs(current_run_results_dir, exist_ok=True)
    if save_each_task_model:
        os.makedirs(current_run_models_dir, exist_ok=True)

    # --- Dataset and Model Misc Init ---
    alphabet, bits_for_model, channels, n_output = init_misc(
        experiment, None, num_concept_features_override=num_concept_features
    )
    collate_fn = get_collate(experiment, device) # For concept learning

    eval_dataset = None
    dataset_source_info = ""
    dataset_iterator = None
    actual_num_tasks_to_evaluate = num_tasks_to_evaluate

    if use_fixed_eval_set and maml_experiment_source:
        print(f"Attempting to load fixed '{use_fixed_eval_set}' dataset generated by MAML.")
        print(f"  MAML source params: exp={maml_experiment_source}, model={maml_model_arch_source}, data={maml_data_type_source}, feats={maml_num_concept_features_source}, depth={maml_pcfg_max_depth_source}, seed={maml_seed_source}")

        dataset_prefix_parts = [maml_experiment_source, maml_model_arch_source, maml_data_type_source]
        if maml_experiment_source == "concept" and maml_data_type_source == "bits":
            dataset_prefix_parts.append(f"feats{maml_num_concept_features_source}")
            dataset_prefix_parts.append(f"depth{maml_pcfg_max_depth_source}")
        elif maml_experiment_source == "mod":
            dataset_prefix_parts.append(f"skip{maml_skip_param_source}")
        elif maml_experiment_source == "omniglot":
            dataset_prefix_parts.append(maml_alphabet_source)
        dataset_prefix_parts.append(f"seed{maml_seed_source}")
        dataset_file_prefix = "_".join(dataset_prefix_parts)

        load_path = None
        if use_fixed_eval_set == "val":
            load_path = os.path.join(SAVED_DATASETS_DIR, f"{dataset_file_prefix}_val_dataset.pt")
        elif use_fixed_eval_set == "test":
            load_path = os.path.join(SAVED_DATASETS_DIR, f"{dataset_file_prefix}_test_dataset.pt")
        else:
            print(f"Warning: Invalid value for --use-fixed-eval-set: {use_fixed_eval_set}. Must be 'val' or 'test'. Falling back.")
            use_fixed_eval_set = None # Trigger fallback

        if load_path:
            try:
                # Call init_dataset to load the specific set.
                # Pass MAML source parameters for context, though loading is by path.
                # n_support=None implies loading the full dataset as saved by main.py.
                # init_dataset returns (train, test, val) if n_support is None.
                _, loaded_test_s, loaded_val_s = init_dataset(
                    experiment=maml_experiment_source, 
                    model_arch=maml_model_arch_source, 
                    data_type=maml_data_type_source, 
                    skip_param=maml_skip_param_source, 
                    alphabet=maml_alphabet_source, # This needs to be the MAML source alphabet
                    num_concept_features=maml_num_concept_features_source,
                    pcfg_max_depth=maml_pcfg_max_depth_source,
                    n_support=None, # Critical: load full datasets as saved by main.py
                    load_train_path=None, # Not loading train set for this baseline script
                    load_val_path=load_path if use_fixed_eval_set == "val" else None,
                    load_test_path=load_path if use_fixed_eval_set == "test" else None
                )
                
                if use_fixed_eval_set == "val":
                    eval_dataset = loaded_val_s
                    dataset_source_info = f"loaded MAML val_dataset ({dataset_file_prefix})"
                elif use_fixed_eval_set == "test":
                    eval_dataset = loaded_test_s
                    dataset_source_info = f"loaded MAML test_dataset ({dataset_file_prefix})"
                
                if eval_dataset is None:
                    raise FileNotFoundError(f"Dataset object for '{use_fixed_eval_set}' from {load_path} was not returned correctly by init_dataset.")

                if not hasattr(eval_dataset, '__getitem__') or not hasattr(eval_dataset, '__len__'):
                    raise TypeError("Loaded eval_dataset is not a standard indexable/iterable PyTorch Dataset.")
                
                print(f"Successfully loaded: {dataset_source_info}, contains {len(eval_dataset)} tasks.")
                actual_num_tasks_to_evaluate = len(eval_dataset)
                fixed_task_loader = DataLoader(eval_dataset, batch_size=1, shuffle=False, collate_fn=collate_fn)
                dataset_iterator = iter(fixed_task_loader)
                
            except FileNotFoundError as e:
                print(f"ERROR: Could not find the MAML dataset at '{load_path}'. Error: {e}")
                print("Please ensure the MAML parameters and seed match a saved dataset. Falling back to on-the-fly generation.")
                use_fixed_eval_set = None # Fallback
            except Exception as e:
                print(f"ERROR: An unexpected error occurred while loading/setting up fixed dataset. Error: {e}")
                print("Falling back to on-the-fly task generation.")
                use_fixed_eval_set = None # Fallback

    if not use_fixed_eval_set: # Fallback or default behavior
        print(f"Using on-the-fly task generation for {actual_num_tasks_to_evaluate} tasks (baseline exp: {experiment}, model: {m}).")
        # For on-the-fly generation, use the baseline's own parameters.
        # init_dataset returns (train, test, val) when n_support is None.
        # We typically use the train_dataset for generating tasks for baselines.
        # The skip_param for 'mod' should be what baseline intends (e.g. 0 or 1).
        # If baseline experiment is 'mod', its own 'skip' should be an arg. For now, hardcode or use a default.
        # For concept, skip_param is not critical. Let's use 0 as a general default if not 'mod'.
        baseline_skip_param = 0 # Default for on-the-fly if not 'mod'
        if experiment == "mod":
            # TODO: Add a `--baseline-skip-param` if necessary for on-the-fly mod generation.
            # For now, let's assume 0 or 1, matching MAML's default if it makes sense.
            baseline_skip_param = 1 # Matching MAML default for skip if baseline is 'mod'
            print(f"  (Using skip_param={baseline_skip_param} for on-the-fly 'mod' generation)")

        on_the_fly_train_ds, _, _ = init_dataset( 
            experiment=experiment, # Baseline's experiment type
            model_arch=m,        # Baseline's model arch
            data_type=data_type,   # Baseline's data type
            skip_param=baseline_skip_param, 
            alphabet=alphabet, # From baseline's init_misc
            num_concept_features=num_concept_features, # Baseline's num_features
            pcfg_max_depth=pcfg_max_depth,           # Baseline's pcfg_max_depth
            n_support=None # Generate a full dataset to sample from
        )
        if on_the_fly_train_ds is None:
            print("ERROR: Failed to generate on-the-fly training dataset. Exiting.")
            return
            
        on_the_fly_loader = DataLoader(
            on_the_fly_train_ds, batch_size=1, shuffle=True, collate_fn=collate_fn, drop_last=True
        )
        dataset_iterator = iter(on_the_fly_loader)
        dataset_source_info = f"on-the-fly generated (exp={experiment}, model={m}) for {actual_num_tasks_to_evaluate} tasks"

    if dataset_iterator is None:
        print("ERROR: Dataset iterator was not initialized. Cannot proceed.")
        return

    if verbose: print(f"Dataset source: {dataset_source_info}")
    all_task_results = []
    
    overall_start_time = time.time()

    for task_idx in range(actual_num_tasks_to_evaluate):
        task_start_time = time.time()
        
        try:
            X_s, y_s, X_q, y_q = next(dataset_iterator)
            X_s, y_s, X_q, y_q = X_s.squeeze(0), y_s.squeeze(0), X_q.squeeze(0), y_q.squeeze(0)
        except StopIteration:
            print(f"Warning: Dataset iterator exhausted at task_idx {task_idx}. Processed {task_idx} tasks.")
            actual_num_tasks_to_evaluate = task_idx # Update to actual number processed
            break
        except Exception as e:
            print(f"ERROR: Failed to get next task data at task_idx {task_idx}. Error: {e}")
            actual_num_tasks_to_evaluate = task_idx
            break

        # --- Initialize a fresh model for this task (using baseline's parameters) ---
        model = init_model(
            m, data_type, index=hyper_index, verbose=False, 
            channels=channels, bits=bits_for_model, n_output=n_output
        ).to(device)

        criterion = nn.BCEWithLogitsLoss() # Assuming concept learning
        optimizer = torch.optim.AdamW(model.parameters(), lr=lr)

        # --- Train on Support Set ---
        if verbose: print(f"  Task {task_idx+1}/{actual_num_tasks_to_evaluate} from {dataset_source_info.split()[0]}: Training... ({num_sgd_steps_per_task} steps)")
        support_losses = train_baseline_on_task(
            model, X_s, y_s, criterion, optimizer, device, num_sgd_steps_per_task
        )
        final_support_loss = support_losses[-1] if support_losses else float('nan')

        # --- Evaluate on Query Set ---
        model.eval()
        with torch.no_grad():
            query_pred = model(X_q)
            query_loss = criterion(query_pred, y_q)
            
            # Calculate accuracy for concept learning (binary classification)
            query_acc = ((torch.sigmoid(query_pred) > 0.5) == y_q.bool()).float().mean().item()

        task_duration = time.time() - task_start_time
        if verbose:
            print(f"    Task {task_idx+1}: QLoss:{query_loss.item():.4f}, QAcc:{query_acc:.4f}, SLoss:{final_support_loss:.4f} ({task_duration:.2f}s)")

        all_task_results.append({
            "task_idx": task_idx,
            "query_loss": query_loss.item(),
            "query_accuracy": query_acc,
            "final_support_loss": final_support_loss,
            "num_sgd_steps": num_sgd_steps_per_task,
            "lr": lr
        })

        # --- Save the task-specific trained model ---
        if save_each_task_model:
            model_filename_parts = [
                experiment, m, str(hyper_index), data_type,
                f"feats{num_concept_features}", f"depth{pcfg_max_depth}",
                f"task{task_idx}", f"run{run_name}", f"seed{seed}"
            ]
            if use_fixed_eval_set: # Add MAML source info to filename if used
                model_filename_parts.append(f"fixed_{use_fixed_eval_set}_mamlseed{maml_seed_source}")
            model_filename = "_".join(model_filename_parts) + ".pt"
            model_save_path = os.path.join(current_run_models_dir, model_filename)
            torch.save(model.state_dict(), model_save_path)
            if verbose: print(f"      Saved task model: {model_save_path}")

    # --- Aggregate and Save Results ---
    avg_query_acc = np.mean([res["query_accuracy"] for res in all_task_results]) if all_task_results else float('nan')
    avg_query_loss = np.mean([res["query_loss"] for res in all_task_results]) if all_task_results else float('nan')
    
    print(f"--- Baseline SGD Summary ({run_name}) ---")
    print(f"Source of tasks: {dataset_source_info}")
    print(f"Experiment: {experiment}, Model: {m}, Features: {num_concept_features}, Depth: {pcfg_max_depth}, Seed: {seed}")
    print(f"Num SGD steps per task: {num_sgd_steps_per_task}, LR: {lr}")
    print(f"Average Query Accuracy over {len(all_task_results)} tasks: {avg_query_acc:.4f}")
    print(f"Average Query Loss over {len(all_task_results)} tasks: {avg_query_loss:.4f}")
    print(f"Total time: {(time.time() - overall_start_time):.2f}s")

    # Save detailed per-task results to CSV
    results_df = pd.DataFrame(all_task_results)
    summary_filename_parts = [
        experiment, m, str(hyper_index), data_type,
        f"feats{num_concept_features}", f"depth{pcfg_max_depth}",
        f"sgdsteps{num_sgd_steps_per_task}", f"lr{lr}", f"run{run_name}", f"seed{seed}"
    ]
    if use_fixed_eval_set: # Add MAML source info to filename if used
        summary_filename_parts.append(f"fixed_{use_fixed_eval_set}_mamlseed{maml_seed_source}")
        summary_filename_parts.append(f"mamlExp{maml_experiment_source}") # Potentially add more maml source details
    summary_filename = "_".join(summary_filename_parts) + "_baselinetrajectory.csv"
    summary_save_path = os.path.join(current_run_results_dir, summary_filename)
    results_df.to_csv(summary_save_path, index=False)
    print(f"Saved baseline SGD trajectory results to {summary_save_path}")


if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="Run Baseline SGD learning for concept tasks.")
    parser.add_argument("--seed", type=int, default=0, help="Random seed.")
    parser.add_argument("--experiment", type=str, default="concept", choices=["concept"], help="Experiment type (fixed to concept for now).")
    parser.add_argument("--m", type=str, default="mlp", choices=["mlp"], help="Baseline model architecture (fixed to mlp for now).")
    parser.add_argument("--data-type", type=str, default="bits", choices=["bits"], help="Baseline data type (fixed to bits for now).")
    
    parser.add_argument("--num-tasks-to-evaluate", type=int, default=100, help="Number of distinct tasks to process (used if not loading fixed set, or as max if loader is shorter).")
    parser.add_argument("--num-sgd-steps-per-task", type=int, default=100, help="Number of SGD steps on the support set of each task.")
    parser.add_argument("--lr", type=float, default=1e-3, help="Learning rate for AdamW optimizer.")
    
    parser.add_argument("--num-concept-features", type=int, default=8, help="Number of features for concept learning.")
    parser.add_argument("--pcfg-max-depth", type=int, default=PCFG_DEFAULT_MAX_DEPTH, help="Max depth for PCFG concept generation.")
    
    parser.add_argument("--no-save-task-models", action="store_false", dest="save_each_task_model", help="Do not save individual task-specific models.")
    parser.add_argument("--hyper-index", type=int, default=DEFAULT_INDEX, help="Hyperparameter index for model initialization.")
    parser.add_argument("--verbose", action="store_true", help="Enable verbose output.")
    parser.add_argument("--run-name", type=str, default="baseline_run1", help="Name for this baseline run (for folder organization).")

    # New arguments for using fixed evaluation sets
    parser.add_argument("--use-fixed-eval-set", type=str, default=None, choices=["val", "test"], help="Specify to use a fixed 'val' or 'test' dataset from a MAML run.")
    parser.add_argument("--maml-experiment-source", type=str, default="concept", help="MAML run's experiment type (e.g., concept)")
    parser.add_argument("--maml-model-arch-source", type=str, default="mlp", help="MAML run's model architecture (e.g., mlp)")
    parser.add_argument("--maml-data-type-source", type=str, default="bits", help="MAML run's data type (e.g., bits)")
    parser.add_argument("--maml-num-concept-features-source", type=int, default=8, help="MAML run's num_concept_features")
    parser.add_argument("--maml-pcfg-max-depth-source", type=int, default=PCFG_DEFAULT_MAX_DEPTH, help="MAML run's pcfg_max_depth")
    parser.add_argument("--maml-seed-source", type=int, default=0, help="MAML run's seed (used for dataset identification)")
    parser.add_argument("--maml-skip-param-source", type=int, default=1, help="MAML run's skip_param (for mod experiment)")
    parser.add_argument("--maml-alphabet-source", type=str, default="asian", help="MAML run's alphabet (for omniglot experiment)")

    args = parser.parse_args()
    main(**vars(args)) 